
Ideas to borrown from pGraph:
- define timezone for specific machines
- define rules of conversion for CPU power, e.g. this processor performs
  20% faster than this one.

Filter per data index context :
- CPU : feed only "all" to database
- IFACE : only relevant interfaces
- etc.

Ideas:

Instead of storing data in columns, which seems to be difficult and more
than a hammer on memory manipulation (close to 50% at data load time),
store in lines, e.g. each metric would have:
- a start time
- a period (computed and/or verified each time, croaking if too much)
- push data in the same table for each metric

Advantages:
- ease restitution if filtering for specific metrics.
- remove the need to precompute column index, as each metric would have
  its own table.
- less storage needed (no timestamps)
- possibly closer to R native timeSeries (ts) concept

Inconvenients:
- not the same baseline for all machines...

That would look like this:

$hdata->{$hostname}->{$index}->{$metric}->{start} = $start_time;
$hdata->{$hostname}->{$index}->{$metric}->{period} = $period;
$context{data_cols} = [ @data_cols ];
...
while (<>) {
    foreach my $d ( @data ) {
        push @{$hdata->{$hostname}->{$index}->{$metric}->{data}}, $d;
    }

    # could be done only once either in inner data read loop, or at
    # start of everything else than data
    $hdata->{$hostname}->{$index}->{$metric}->{last} = $tstamp;
}

Not sure we can have a global notation.


Question: how do we handle data blocks on the same metric, which have
multiple days ?
--> store current last tstamp for each metric

